# ============ Public Configuration ============
public:
  env_dim: &env_dim 16
  action_dim: &action_dim 4
  env: game2048
  agent: dqn
  model: resnet
  trainer: base
  device: cuda:0
  from_checkpoint: "" # Path to load a checkpoint

# ============ Deep Learning Model Configuration ============
model:
  # multi-layer perceptron
  mlp:
    input_len: *env_dim
    embedding_dim: 16
    num_embeddings: 32
    feed_forward:
      num_layers: 2
      hidden_dim: 32
      output_dim: *action_dim
      activation: "relu"
      bias: true

  # resnet model
  resnet:
    input_len: *env_dim
    input_height: 4
    input_width: 4
    embedding_dim: &resnet_embedding_dim 16
    num_embeddings: 32
    residual_block:
      num_blocks: 2
      kernel_size: 3
      in_channels: *resnet_embedding_dim
      out_channels: *resnet_embedding_dim
      stride: 1
      padding: 1
      activation: "relu"
    output_dim: *action_dim

# ============ Environment Configuration ============
env:
  # 2048 game
  game2048:
    grid_num: *env_dim
    grid_size: 4
    new_tile_value:
      2: 0.7
      4: 0.3
    style:
      tile_size: 100
      font_size: 40
      grid_padding: 5
      background_color: [250, 248, 239]
      font_color: [119, 110, 101]
      btn_color: [143, 122, 102]
      tile_colors:
        0: [205, 193, 180]
        2: [238, 228, 218]
        4: [237, 224, 200]
        8: [242, 177, 121]
        16: [245, 149, 99]
        32: [246, 124, 95]
        64: [246, 94, 59]
        128: [237, 207, 114]
        256: [237, 204, 97]
        512: [237, 200, 80]
        1024: [237, 197, 63]
        2048: [237, 194, 46]
      font_colors:
        2: [119, 110, 101]
        4: [119, 110, 101]
        8: [255, 255, 255]
        16: [255, 255, 255]
        32: [255, 255, 255]
        64: [255, 255, 255]
        128: [255, 255, 255]
        256: [255, 255, 255]
        512: [255, 255, 255]
        1024: [255, 255, 255]
        2048: [255, 255, 255]
    archive_file: envs/game2048.dat

# ============ Trainer Configuration ============
trainer:
  base:
    exp_name: default
    batch_size: 16
    episode: &episode 10000
    episode_max_step: 1000
    replay_buffer_size: 10000
    replay_buffer_size_min: 100
    learning_rate:
      warmup_rate: 0.1
      total_steps: *episode
      eta_min: 1.0e-06
      eta_max: 2.5e-05
    optimizer: Adam
    log_interval: 100
    save_interval: 1000
    output_dir: "outputs/"

# ============ RL Agent Configuration ============
agent:
  dqn:
    gamma: 0.95
    action_space: *action_dim
    strategy: online
    target_network:
      use: true
      update_step: 20
      update_method: hard # hard or soft
      update_soft_tau: 0.8
    online:
      start_epsilon: 1.0
      end_epsilon: 0.05
      epsilon_decay: 1000000
    offline:
      action_logit:
        - 0.25
        - 0.25
        - 0.25
        - 0.25