# ============ Public Configuration ============
public:
  env_dim: &env_dim 16
  action_dim: &action_dim 4
  env: game2048
  agent: dqn
  model: mlp
  trainer: base
  device: cuda:0
  from_checkpoint: "" # Path to load a checkpoint

# ============ Environment Configuration ============
env:
  # 2048 game
  game2048:
    grid_num: *env_dim
    grid_size: 4
    new_tile_value:
      2: 0.7
      4: 0.3
    style:
      tile_size: 100
      font_size: 40
      grid_padding: 5
      background_color: [250, 248, 239]
      font_color: [119, 110, 101]
      btn_color: [143, 122, 102]
      tile_colors:
        0: [205, 193, 180]
        2: [238, 228, 218]
        4: [237, 224, 200]
        8: [242, 177, 121]
        16: [245, 149, 99]
        32: [246, 124, 95]
        64: [246, 94, 59]
        128: [237, 207, 114]
        256: [237, 204, 97]
        512: [237, 200, 80]
        1024: [237, 197, 63]
        2048: [237, 194, 46]
      font_colors:
        2: [119, 110, 101]
        4: [119, 110, 101]
        8: [255, 255, 255]
        16: [255, 255, 255]
        32: [255, 255, 255]
        64: [255, 255, 255]
        128: [255, 255, 255]
        256: [255, 255, 255]
        512: [255, 255, 255]
        1024: [255, 255, 255]
        2048: [255, 255, 255]
    archive_file: envs/game2048.dat

# ============ RL Agent Configuration ============
agent:
  dqn:
    gamma: 0.99
    action_space: *action_dim
    strategy: online
    target_network:
      use: true
      update_step: 500
      update_method: hard # hard or soft
      update_soft_tau: 0.8
    online:
      start_epsilon: 1.0
      end_epsilon: 0.05
      epsilon_decay: 250000
    offline:
      action_logit:
        - 0.25
        - 0.25
        - 0.25
        - 0.25
# ============ Deep Learning Model Configuration ============

model:
  # multi-layer perceptron
  mlp:
    input_len: *env_dim
    input_embedding:
      embedding_dim: 16
      num_embeddings: 32
    position_embedding:
      max_len: *env_dim
      embedding_dim: 16
      mode: "sinusoidal" # "sinusoidal" or "learnable"
      method: "add" # "add" or "concat"
    feed_forward:
      num_layers: 4
      hidden_dim: 64
      output_dim: *action_dim
      activation: "gelu"
      bias: true

  # resnet model
  resnet:
    input_len: *env_dim
    input_height: 4
    input_width: 4
    input_embedding:
      embedding_dim: &resnet_embedding_dim 16
      num_embeddings: 32
    position_embedding:
      max_len: *env_dim
      embedding_dim: 16
      mode: "sinusoidal" # "sinusoidal" or "learnable"
      method: "add" # "add" or "concat"
    residual_block:
      num_blocks: 3
      kernel_size: 3
      stride: 1
      padding: 1
      hidden_dim: 64
    activation: "gelu"
    output_dim: *action_dim

# ============ Trainer Configuration ============
trainer:
  base:
    exp_name: mlp-small
    batch_size: 32
    episode: &episode 50000
    episode_max_step: 1000
    replay_buffer_size: 50000
    replay_buffer_size_min: 1000
    learning_rate:
      warmup_rate: 0.1
      total_steps: *episode
      eta_min: 0.0
      eta_max: 1.0e-04
    optimizer: Adam
    log_interval: 50
    save_interval: 1000
    output_dir: "outputs/"

