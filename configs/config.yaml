# ============ Public Configuration ============
public:
  env_dim: &env_dim 16
  action_dim: &action_dim 4
  env: game2048
  agent: imitation
  model: mlp
  trainer: imitation
  device: cpu
  from_checkpoint: "" # test

# ============ Trainer Configuration ============
trainer:
  # DQN Trainer
  dqn:
    exp_name: mlp-small-dqn
    batch_size: 64
    train_episode: &train_episode 20000
    episode_max_step: 5000
    network_update_interval: 5
    replay_buffer_update_interval: 1
    replay_buffer_size: 100000
    replay_buffer_size_min: 10000
    learning_rate:
      warmup_rate: 0.1
      total_steps: *train_episode
      eta_min: 1.0e-7
      eta_max: 5.0e-05
    optimizer: AdamW
    log_interval: 100
    save_interval: 1000
    output_dir: "outputs/"
    from_checkpoint: "" # Path to load a checkpoint

  # Imitation Learning Trainer
  imitation:
    exp_name: mlp-small-imitation
    data_file: "data/human_2048.json" # Path to imitation learning data file
    batch_size: 64
    train_epoch: 100
    episode_max_step: 5000
    learning_rate:
      warmup_rate: 0.1
      total_steps: *train_episode
      eta_min: 1.0e-7
      eta_max: 5.0e-05
    optimizer: AdamW
    log_interval: 1
    save_interval: 10
    output_dir: "outputs/"
    from_checkpoint: "" # Path to load a checkpoint

# ============ Environment Configuration ============
env:
  # 2048 game
  game2048:
    grid_num: *env_dim
    grid_size: 4
    new_tile_value:
      2: 0.7
      4: 0.3
    style:
      tile_size: 100
      font_size: 40
      grid_padding: 5
      background_color: [250, 248, 239]
      font_color: [119, 110, 101]
      btn_color: [143, 122, 102]
      tile_colors:
        0: [205, 193, 180]
        2: [238, 228, 218]
        4: [237, 224, 200]
        8: [242, 177, 121]
        16: [245, 149, 99]
        32: [246, 124, 95]
        64: [246, 94, 59]
        128: [237, 207, 114]
        256: [237, 204, 97]
        512: [237, 200, 80]
        1024: [237, 197, 63]
        2048: [237, 194, 46]
      font_colors:
        2: [119, 110, 101]
        4: [119, 110, 101]
        8: [255, 255, 255]
        16: [255, 255, 255]
        32: [255, 255, 255]
        64: [255, 255, 255]
        128: [255, 255, 255]
        256: [255, 255, 255]
        512: [255, 255, 255]
        1024: [255, 255, 255]
        2048: [255, 255, 255]
    archive_file: envs/game2048.dat

# ============ RL Agent Configuration ============
agent:
  # Deep Q-Network Agent
  dqn:
    gamma: 0.99
    action_space: *action_dim
    strategy: online
    target_network:
      use: true
      update_step: 1000
      update_method: hard # hard or soft
      update_soft_tau: 0.8
    online:
      start_epsilon: 1.0
      end_epsilon: 0.05
      epsilon_decay: 15000
    offline:
      data_file: "" # Path to offline data file
      action_logit:
        - 0.25
        - 0.25
        - 0.25
        - 0.25

  # Imitation Learning Agent
  imitation:
    action_space: *action_dim

# ============ Deep Learning Model Configuration ============
model:
  # multi-layer perceptron
  mlp:
    input_len: *env_dim
    input_embedding:
      embedding_dim: 32
      num_embeddings: 32
    position_embedding:
      max_len: *env_dim
      embedding_dim: 32
      mode: "sinusoidal" # "sinusoidal" or "learnable"
      method: "add" # "add" or "concat"
    feed_forward:
      num_layers: 3
      hidden_dim: 64
      output_dim: *action_dim
      activation: "gelu"
      bias: true

  # resnet model
  resnet:
    input_len: *env_dim
    input_height: 4
    input_width: 4
    input_embedding:
      embedding_dim: &resnet_embedding_dim 16
      num_embeddings: 32
    position_embedding:
      max_len: *env_dim
      embedding_dim: 16
      mode: "sinusoidal" # "sinusoidal" or "learnable"
      method: "add" # "add" or "concat"
    residual_block:
      num_blocks: 3
      kernel_size: 3
      stride: 1
      padding: 1
      hidden_dim: 64
    activation: "gelu"
    output_dim: *action_dim

  # transformer encoder
  transformer:
    input_len: *env_dim
    input_embedding:
      embedding_dim: 32
      num_embeddings: 32
    position_embedding:
      max_len: *env_dim
      embedding_dim: 32
      mode: "sinusoidal" # "sinusoidal" or "learnable"
      method: "add" # "add" or "concat"
    block:
      num_layers: 3
      num_heads: 3
      ffn_hidden_dim: 64
      ffn_num_layers: 2
      norm_type: "post" # "pre" or "post"
      dropout: 0.0
      activation: "gelu"
      bias: false
    output_dim: *action_dim
